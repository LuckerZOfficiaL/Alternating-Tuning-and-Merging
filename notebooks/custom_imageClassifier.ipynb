{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook re-implements the image classifier class with the integration of task vector threshold clipping. In order for this notebook to work properly, the encoder class must also be re-implemented, which I have done already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "import numpy as np\n",
    "\n",
    "from tvp import utils\n",
    "from tvp.utils.utils import torch_load, torch_save\n",
    "\n",
    "\n",
    "class ImageEncoder(torch.nn.Module):\n",
    "    def __init__(self, model_name: str, openclip_cachedir=None, cache_dir=None, keep_lang=False, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(f\"Loading {model_name} pre-trained weights.\")\n",
    "        if \"__pretrained__\" in model_name:\n",
    "            name, pretrained = model_name.split(\"__pretrained__\")\n",
    "        else:\n",
    "            name = model_name\n",
    "            pretrained = \"openai\"\n",
    "\n",
    "        self.model, self.train_preprocess, self.val_preprocess = open_clip.create_model_and_transforms(\n",
    "            name, pretrained=pretrained, cache_dir=openclip_cachedir\n",
    "        )\n",
    "\n",
    "        self.pretrained_state_dict = self.get_pretrained_weights()\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        if not keep_lang and hasattr(self.model, \"transformer\"):\n",
    "            delattr(self.model, \"transformer\")\n",
    "\n",
    "        # NOTE excluding the classification head\n",
    "        # TODO eval whether it should be included as well\n",
    "        self.MODULE_NAMES_ELIGIBLE_FOR_FREEZING = [\n",
    "            \"conv1\",\n",
    "            \"ln_pre\",\n",
    "            \"ln_1\",\n",
    "            \"ln_2\",\n",
    "            \"c_fc\",\n",
    "            \"c_proj\",\n",
    "            \"ln_post\",\n",
    "            \"ln_final\",\n",
    "            \"token_embedding\",\n",
    "            \"out_proj\",  # gotta properly handle it (https://github.com/pytorch/pytorch/issues/69353 <3) to prevent RuntimeError\n",
    "        ]\n",
    "\n",
    "    def forward(self, images):\n",
    "        assert self.model is not None\n",
    "        return self.model.encode_image(images)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.forward(inputs)\n",
    "\n",
    "    def save(self, filename):\n",
    "        print(f\"Saving image encoder to {filename}\")\n",
    "        utils.torch_save(self, filename)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name, filename):\n",
    "        print(f\"Loading image encoder from {filename}\")\n",
    "        state_dict = torch.load(filename)\n",
    "        return cls.load(model_name, state_dict)\n",
    "    \n",
    "    \n",
    "    def get_pretrained_weights(self):\n",
    "        return {name: param.detach().clone() for name, param in self.model.named_parameters()}\n",
    "\n",
    "    def save_pretrained_weights_to_file(self, filename):\n",
    "        torch.save(self.pretrained_weights, filename)\n",
    "        print(f\"Pretrained weights saved to {filename}\")\n",
    "\n",
    "    def reset_weights_by_thresh(self, threshold=1e-4):\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if name in self.pretrained_weights:\n",
    "                    change_magnitude = torch.abs(param - self.pretrained_weights[name]).max()\n",
    "                    if change_magnitude < threshold:\n",
    "                        param.data.copy_(self.pretrained_weights[name])\n",
    "\n",
    "    def compute_threshold_by_percentile(self, percentile):\n",
    "        changes = []\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if name in self.pretrained_state_dict:\n",
    "                    pretrained_param = self.pretrained_state_dict[name].to(param.device)\n",
    "                    change_magnitude = torch.abs(param - pretrained_param)\n",
    "                    changes.append(change_magnitude.view(-1))\n",
    "        changes = torch.cat(changes)\n",
    "        threshold = np.percentile(changes.cpu().numpy(), 100 - percentile * 100)\n",
    "        return threshold\n",
    "\n",
    "    def reset_weights_by_percentile(self, percentile):\n",
    "        threshold = self.compute_threshold_by_percentile(percentile)\n",
    "        self.reset_weights_by_thresh(threshold)\n",
    "\n",
    "\n",
    "    def get_tv_sparsity(self):\n",
    "        current_state_dict = self.model.state_dict()\n",
    "        total_params = 0\n",
    "        unchanged_params = 0\n",
    "\n",
    "        for name, current_param in current_state_dict.items():\n",
    "            pretrained_param = self.pretrained_state_dict[name]\n",
    "            total_params += current_param.numel()\n",
    "            unchanged_params += torch.sum(current_param == pretrained_param).item()\n",
    "\n",
    "        unchanged_percentage = (unchanged_params / total_params) * 100\n",
    "        #print(f\"Percentage of parameters that remained pretrained: {unchanged_percentage:.2f}%\")\n",
    "        return 100-unchanged_percentage\n",
    "\n",
    "    # @classmethod\n",
    "    # def load_from_state_dict(cls, model_name, state_dict):\n",
    "    #     cls.model, cls.train_preprocess, cls.val_preprocess = open_clip.create_model_and_transforms(\n",
    "    #         model_name, pretrained=pretrained, cache_dir=args.openclip_cachedir\n",
    "    #     )\n",
    "    #     cls.model.load_from_state_dict(state_dict)\n",
    "\n",
    "    # NOTE this and the following base result in the same percentage of frozen params for each layer\n",
    "    # TODO eval what is the best approach and keep only one\n",
    "    # https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#pruning-a-module\n",
    "    # def pick_params_to_prune_by_layers(self, pct: float):\n",
    "    #     for name, module in self.named_modules():\n",
    "    #         if not hasattr(module, \"weight\"):\n",
    "    #             continue\n",
    "\n",
    "    #         if not any(substring in name for substring in self.MODULE_NAMES_ELIGIBLE_FOR_FREEZING):\n",
    "    #             continue\n",
    "\n",
    "    #         prune.random_unstructured(module, name=\"weight\", amount=pct)\n",
    "\n",
    "    # # https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#global-pruning\n",
    "    # def pick_params_to_prune_by_nn(self, pct: float):\n",
    "    #     modules_to_freeze = []\n",
    "\n",
    "    #     for name, module in self.named_modules():\n",
    "    #         if not hasattr(module, \"weight\"):\n",
    "    #             continue\n",
    "\n",
    "    #         if not any(substring in name for substring in self.MODULE_NAMES_ELIGIBLE_FOR_FREEZING):\n",
    "    #             continue\n",
    "\n",
    "    #         modules_to_freeze.append((module, \"weight\"))\n",
    "\n",
    "    #     prune.global_unstructured(\n",
    "    #         modules_to_freeze,\n",
    "    #         pruning_method=prune.RandomUnstructured,\n",
    "    #         amount=pct,\n",
    "    #     )\n",
    "\n",
    "    # # NOTE alternative name: remove_pruning_metadata\n",
    "    # def make_pruning_effective(self):\n",
    "    #     for name, module in self.named_modules():\n",
    "    #         if not hasattr(module, \"weight\"):\n",
    "    #             continue\n",
    "\n",
    "    #         if not any(substring in name for substring in self.MODULE_NAMES_ELIGIBLE_FOR_FREEZING):\n",
    "    #             continue\n",
    "\n",
    "    #         prune.remove(module, \"weight\")\n",
    "\n",
    "\n",
    "class ClassificationHead(torch.nn.Linear):\n",
    "    def __init__(self, normalize, input_size=None, num_classes=None, weights=None, biases=None, **kwargs):\n",
    "        assert (input_size is not None and num_classes is not None) or weights is not None\n",
    "\n",
    "        if weights is not None:\n",
    "            num_classes, input_size = weights.shape\n",
    "\n",
    "        super().__init__(in_features=input_size, out_features=num_classes)\n",
    "        self.normalize = normalize\n",
    "        if weights is not None:\n",
    "            self.weight = torch.nn.Parameter(weights.clone())\n",
    "        if biases is not None:\n",
    "            self.bias = torch.nn.Parameter(biases.clone())\n",
    "        else:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros_like(self.bias))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.normalize:\n",
    "            inputs = inputs / inputs.norm(dim=-1, keepdim=True)\n",
    "        return super().forward(inputs)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.forward(inputs)\n",
    "\n",
    "    def save(self, filename):\n",
    "        print(f\"Saving classification head to {filename}\")\n",
    "        torch_save(self, filename)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        print(f\"Loading classification head from {filename}\")\n",
    "        return torch_load(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
